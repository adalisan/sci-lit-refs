@article{Shi,
author = {Shi, Xiaoxiao and Fan, Wei and Ren, Jiangtao},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi, Fan, Ren - Unknown - Actively transfer domain knowledge.pdf:pdf},
journal = {Machine Learning and Knowledge Discovery in Databases},
number = {60703110},
pages = {342--357},
publisher = {Springer},
title = {{Actively transfer domain knowledge}},
url = {http://www.springerlink.com/index/c807375717r1378q.pdf}
}
@article{Caruana1997,
author = {Caruana, R.},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Caruana - 1997 - Multitask learning.pdf:pdf},
journal = {Machine Learning},
keywords = {backpropagation,generalization,inductive transfer,k-nearest neighbor,kernel,multitask learning,parallel transfer,regression,supervised learning},
number = {1},
pages = {41--75},
publisher = {Springer},
title = {{Multitask learning}},
url = {http://www.springerlink.com/index/X4Q010H7342J4P15.pdf},
volume = {28},
year = {1997}
}
@article{Settles2009,
author = {Settles, Burr},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Settles - 1994 - Active Learning Literature Survey\$\$.pdf:pdf},
journal = {Machine Learning},
number = {2},
pages = {201--221},
publisher = {Citeseer},
title = {{Active Learning Literature Survey\$\backslash\$}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.4245\&amp;rep=rep1\&amp;type=pdf},
volume = {15},
year = {1994}
}
@article{Sing2007,
author = {Sing, Tobias and Sander, Oliver and Beerenwinkel, Niko and Lengauer, Thomas},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sing et al. - 2007 - The ROCR Package.pdf:pdf},
title = {{The ROCR Package}},
year = {2007}
}
@article{Sun2005,
author = {Sun, Quan-sen and Zeng, Sheng-gen and Liu, Yan and Heng, Pheng-ann and Xia, De-shen},
doi = {10.1016/j.patcog.2004.12.013},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun et al. - 2005 - A new method of feature fusion and its application in image recognition.pdf:pdf},
journal = {Pattern Recognition},
keywords = {canonical correlation analysis,cca,face recognition,feature extraction,feature fusion,handwritten character recognition},
pages = {2437 -- 2448},
title = {{A new method of feature fusion and its application in image recognition}},
volume = {38},
year = {2005}
}
@article{Lafferty2005,
author = {Lafferty, John and Lebanon, Guy},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lafferty, Lebanon - 2006 - Diffusion kernels on statistical manifolds.pdf:pdf},
issn = {1533-7928},
journal = {Journal of Machine Learning Research},
keywords = {manifolds},
mendeley-tags = {manifolds},
number = {1},
pages = {129},
publisher = {Citeseer},
title = {{Diffusion kernels on statistical manifolds}},
type = {Journal article},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.61.1896\&amp;rep=rep1\&amp;type=pdf},
volume = {6},
year = {2006}
}
@inproceedings{Gong2005,
abstract = {This paper presents a novel scheme for manifold learning. Different from the previous work reducing data to Euclidean space which cannot handle the looped manifold well, we map the scattered data to its intrinsic parameter manifold by semisupervised learning. Given a set of partially labeled points, the map to a specified parameter manifold is computed by an iterative neighborhood average method called anchor points diffusion procedure (APD). We explore this idea on the most frequently used close formed manifolds, Stiefel manifolds whose special cases include hyper sphere and orthogonal group. The experiments show that APD can recover the underlying intrinsic parameters of points on scattered data manifold successfully.},
author = {Gong, Haifeng and Pan, Chunhong and Yang, Qing and Lu, Hanqing and Ma, Songde},
booktitle = {Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on},
keywords = {manifolds},
mendeley-tags = {manifolds},
pages = {98--105 Vol. 1},
title = {{A semi-supervised framework for mapping data to the intrinsic manifold}},
type = {Conference proceedings (whole)},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1541244},
volume = {1},
year = {2005}
}
@article{Carter2009,
abstract = {We consider the problems of clustering, classification, and visualization of high-dimensional data when no straightforward euclidean representation exists. In this paper, we propose using the properties of information geometry and statistical manifolds in order to define similarities between data sets using the Fisher information distance. We will show that this metric can be approximated using entirely nonparametric methods, as the parameterization and geometry of the manifold is generally unknown. Furthermore, by using multidimensional scaling methods, we are able to reconstruct the statistical manifold in a low-dimensional euclidean space; enabling effective learning on the data. As a whole, we refer to our framework as Fisher Information Nonparametric Embedding (FINE) and illustrate its uses on practical problems, including a biomedical application and document classification.},
author = {Carter, Kevin M. and Raich, Raviv and Finn, William G. and III, Alfred O. Hero},
doi = {10.1109/TPAMI.2009.67},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carter et al. - 2009 - FINE Fisher Information Nonparametric Embedding.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {embedding,fisher,information,manifolds},
mendeley-tags = {embedding,fisher,information,manifolds},
month = apr,
number = {11},
pages = {2093--2098},
publisher = {IEEE Computer Society},
title = {{FINE: Fisher Information Nonparametric Embedding}},
type = {Journal article},
url = {http://doi.ieeecomputersociety.org/10.1109/TPAMI.2009.67 http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4815255},
volume = {31},
year = {2009}
}
@inproceedings{Crammer2007,
author = {Crammer, K and Kearns, M and Wortman, J},
booktitle = {Neural Information Processing Systems},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crammer, Kearns, Wortman - 2007 - Learning from multiple sources.pdf:pdf},
title = {{Learning from multiple sources}},
volume = {9},
year = {2007}
}
@article{Ben-David2007,
author = {Ben-David, Shai and Blitzer, J and Crammer, K and Pereira, F},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben-David et al. - 2007 - Analysis of representations for domain adaptation.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {137},
publisher = {Citeseer},
title = {{Analysis of representations for domain adaptation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.7478\&amp;rep=rep1\&amp;type=pdf},
volume = {19},
year = {2007}
}
@inproceedings{Pan2009,
annote = {undefined},
author = {Pan, S.J. and Tsang, I.W. and Kwok, J.T. and Yang, Q.},
booktitle = {Proceedings of the 21st International Joint Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan et al. - 2009 - Domain adaptation via transfer component analysis.pdf:pdf},
title = {{Domain adaptation via transfer component analysis}},
url = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI-09/paper/download/294/962},
year = {2009}
}
@misc{,
annote = {undefined},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Robust Distance Metric Learning with Auxiliary Knowledge.pdf:pdf},
keywords = {distance metric,transfer learning},
mendeley-tags = {distance metric,transfer learning},
title = {{Robust Distance Metric Learning with Auxiliary Knowledge}},
url = {http://ijcai.org/papers09/Papers/IJCAI09-223.pdf}
}
@inproceedings{Dai2009,
annote = {undefined},
author = {Dai, W. and Jin, O. and Xue, G.R. and Yang, Q. and Yu, Y.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai et al. - 2009 - Eigentransfer a unified framework for transfer learning.pdf:pdf},
keywords = {transfer learning},
mendeley-tags = {transfer learning},
pages = {193--200},
publisher = {ACM},
title = {{Eigentransfer: a unified framework for transfer learning}},
url = {http://portal.acm.org/citation.cfm?id=1553374.1553399},
year = {2009}
}
@article{Zhong2009,
abstract = {When labeled examples are limited and difficult to obtain, transfer learning employs knowledge from a source domain to improve learning accuracy in the target domain. However, the assumption made by existing approaches, that the marginal and conditional probabilities are directly related between source and target domains, has limited applicability in either the original space or its linear transformations. To solve this problem, we propose an adaptive kernel approach that maps the marginal distribution of target-domain and source-domain data into a common kernel space, and utilize a sample selection strategy to draw conditional probabilities between the two domains closer. We formally show that under the kernel-mapping space, the difference in distributions between the two domains is bounded; and the prediction error of the proposed approach can also be bounded. Experimental results demonstrate that the proposed method outperforms both traditional inductive classifiers and the state-of-the-art boosting-based transfer algorithms on most domains, including text categorization and web page ratings. In particular, it can achieve around 10\% higher accuracy than other approaches for the text categorization problem. The source code and datasets are available from the authors.},
author = {Zhong, Erheng and Fan, Wei and Peng, Jing and Zhang, Kun and Ren, Jiangtao and Turaga, Deepak and Verscheure, Olivier},
journal = {International Conference on Knowledge Discovery and Data Mining},
keywords = {domain transfer,ensemble,generalization bound,kernel},
title = {{Cross domain distribution adaptation via kernel mapping}},
url = {http://portal.acm.org/citation.cfm?id=1557019.1557130},
year = {2009}
}
@misc{,
annote = {undefined},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Knowledge Transfer via Multiple Model Local Structure Mapping.html:html},
title = {{Knowledge Transfer via Multiple Model Local Structure Mapping}},
url = {http://www.ews.uiuc.edu/~jinggao3/doc/kdd08-gao.pdf}
}
@article{Amini2009,
abstract = {Abstract  We address the problem of learning text categorization from a corpus of multilingual documents. We propose a multiview learning,
  co-regularization approach, in which we consider each language as a separate source, and minimize a joint loss that combines
  monolingual classification losses in each language while ensuring consistency of the categorization across languages. We derive
  training algorithms for logistic regression and boosting, and show that the resulting categorizers outperform models trained
  independently on each language, and even, most of the times, models trained on the joint bilingual data. Experiments are carried
  out on a multilingual extension of the RCV2 corpus, which is available for benchmarking.},
author = {Amini, Massih-Reza and Goutte, Cyril},
doi = {10.1007/s10994-009-5151-5},
issn = {0885-6125},
journal = {Machine Learning},
month = may,
number = {1-2},
pages = {105--121},
title = {{A co-classification approach to learning from multilingual corpora}},
url = {http://www.springerlink.com/content/j8l1445j04x20703},
volume = {79},
year = {2009}
}
@article{Pan2008,
abstract = {Transfer learning addresses the problem of how to utilize plenty of labeled data in a source domain to solve related but different problems in a target domain, even when the training and testing problems have different distributions or features. In this paper, we consider transfer learning via dimensionality reduction. To solve this problem, we learn a low-dimensional latent feature space where the distributions between the source domain data and the target domain data are the same or close to each other. Onto this latent feature space, we project the data in related domains where we can apply standard learning algorithms to train classification or regression models. Thus, the latent feature space can be treated as a bridge of transferring knowledge from the source domain to the target domain. The main contribution of our work is that we propose a new dimensionality reduction method to find a latent space, which minimizes the distance between distributions of the data in different domains in a latent space. The effectiveness of our approach to transfer learning is verified by experiments in two real world applications: indoor WiFi localization and binary text classification.},
author = {Pan, Sinno Jialin and Kwok, James T. and Yang, Qiang},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan, Kwok, Yang - 2008 - Transfer learning via dimensionality reduction.pdf:pdf},
journal = {Aaai Conference On Artificial Intelligence},
title = {{Transfer learning via dimensionality reduction}},
url = {http://portal.acm.org/citation.cfm?id=1620177},
year = {2008}
}
@article{Ling2008,
abstract = {Traditional spectral classification has been proved to be effective in dealing with both labeled and unlabeled data when these data are from the same domain. In many real world applications, however, we wish to make use of the labeled data from one domain (called in-domain) to classify the unlabeled data in a different domain (out-of-domain). This problem often happens when obtaining labeled data in one domain is difficult while there are plenty of labeled data from a related but different domain. In general, this is a transfer learning problem where we wish to classify the unlabeled data through the labeled data even though these data are not from the same domain. In this paper, we formulate this domain-transfer learning problem under a novel spectral classification framework, where the objective function is introduced to seek consistency between the in-domain supervision and the out-of-domain intrinsic structure. Through optimization of the cost function, the label information from the in-domain data is effectively transferred to help classify the unlabeled data from the out-of-domain. We conduct extensive experiments to evaluate our method and show that our algorithm achieves significant improvements on classification performance over many state-of-the-art algorithms.},
author = {Ling, Xiao and Dai, Wenyuan and Xue, Gui-Rong and Yang, Qiang and Yu, Yong},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ling et al. - 2008 - Spectral domain-transfer learning.pdf:pdf},
journal = {International Conference on Knowledge Discovery and Data Mining},
keywords = {spectral learning,transfer learning},
pages = {488--496},
title = {{Spectral domain-transfer learning}},
url = {http://portal.acm.org/citation.cfm?id=1401951},
year = {2008}
}
@article{Dai2007,
abstract = {Traditional machine learning makes a basic assumption: the training and test data should be under the same distribution. However, in many cases, this identical-distribution assumption does not hold. The assumption might be violated when a task from one new domain comes, while there are only labeled data from a similar old domain. Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this paper, we present a novel transfer learning framework called TrAdaBoost, which extends boosting-based learning algorithms (Freund \& Schapire, 1997). TrAdaBoost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. We show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data, even when the new data are not sufficient to train a model alone. We show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new. The effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model.},
author = {Dai, Wenyuan and Yang, Qiang and Xue, Gui-Rong and Yu, Yong},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai et al. - 2007 - Boosting for transfer learning.pdf:pdf},
journal = {ICML; Vol. 227},
pages = {193},
title = {{Boosting for transfer learning}},
url = {http://portal.acm.org/citation.cfm?id=1273521},
year = {2007}
}
@article{Si2010,
abstract = {Is it possible to train a learning model to separate tigers from elks when we have 1) labeled samples of leopard and zebra and 2) unlabelled samples of tiger and elk at hand? Cross-domain learning algorithms can be used to solve the above problem. However, existing cross-domain algorithms cannot be applied for dimension reduction, which plays a key role in computer vision tasks, e.g., face recognition and web image annotation. This paper envisions the cross-domain discriminative dimension reduction to provide an effective solution for cross-domain dimension reduction. In particular, we propose the cross-domain discriminative Hessian Eigenmaps or CDHE for short. CDHE connects training and test samples by minimizing the quadratic distance between the distribution of the training set and that of the test set. Therefore, a common subspace for data representation can be well preserved. Furthermore, we basically expect the discriminative information used to separate leopards and zebra can be shared to separate tigers and elks, and thus we have a chance to duly address the above question. Margin maximization principle is adopted in CDHE so the discriminative information for separating different classes (e.g., leopard and zebra here) can be well preserved. Finally, CDHE encodes the local geometry of each training class (e.g., leopard and zebra here) in the local tangent space which is locally isometric to the data manifold and thus CDHE preserves the intraclass local geometry. The objective function of CDHE is not convex, so the gradient descent strategy can only find a local optimal solution. In this paper, we carefully design an evolutionary search strategy to find a better solution of CDHE. Experimental evidence on both synthetic and real word image datasets demonstrates the effectiveness of CDHE for cross-domain web image annotation and face recognition.},
author = {Si, Si and Tao, Dacheng and Chan, Kwok-Ping},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Si, Tao, Chan - 2010 - Evolutionary cross-domain discriminative hessian eigenmaps.pdf:pdf},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
keywords = {cross-domain learning,dimension reduction,evolutionary search,face recognition,manifold learning,web image annotation},
number = {4},
pages = {1075--1086},
title = {{Evolutionary cross-domain discriminative hessian eigenmaps}},
url = {http://portal.acm.org/citation.cfm?id=1820776.1820795},
volume = {19},
year = {2010}
}
@book{Mardia1980,
author = {Mardia, Kanti V. and Kent, J. T. and Bibby, J. M.},
isbn = {0124712525},
pages = {521},
publisher = {Academic Press},
title = {{Multivariate Analysis (Probability and Mathematical Statistics)}},
url = {http://www.amazon.com/Multivariate-Analysis-Probability-Mathematical-Statistics/dp/0124712525},
year = {1980}
}
@article{Bjoerck1971,
abstract = {Assume that two subspaces F and G of unitary space are defined as the ranges (or nullspaces) of given rectangular matrices A and B. Accurate numerical methods are developed for computing the principal angles \$\backslash theta\_k (F,G)\$ and orthogonal sets of principal vectors \$u\_k\backslash \backslash epsilon\backslash F\$ and \$v\_k\backslash \backslash epsilon\backslash G\$, k = 1,2,..., q = dim(G) \$\backslash leq\$ dim(F). An important application in statistics is computing the canonical correlations \$\backslash sigma\_k\backslash = cos \backslash theta\_k\$ between two sets of variates. A perturbation analysis shows that the condition number for \$\backslash theta\_k\$ essentially is max(\$\backslash kappa (A),\backslash kappa (B)\$), where \$\backslash kappa\$ denotes the condition number of a matrix. The algorithms are based on a preliminary QR-factorization of A and B (or \$A\^{}H\$ and \$B\^{}H\$), for which either the method of Householder transformations (HT) or the modified Gram-Schmidt method (MGS) is used. Then cos \$\backslash theta\_k\$ and sin \$\backslash theta\_k\$ are computed as the singular values of certain related matrices. Experimental results are given, which indicates that MGS gives \$\backslash theta\_k\$ with equal precision and fewer arithmetic operations than HT. However, HT gives principal vectors, which are orthogonal to working accuracy, which is not in general true for MGS. Finally the case when A and/or B are rank deficient is discussed.},
author = {Bjoerck, Ake and Golub, Gene H.},
title = {{Numerical methods for computing angles between linear subspaces}},
url = {http://portal.acm.org/citation.cfm?id=891913},
year = {1971}
}
@article{Rakocevic2003,
abstract = {Canonical angles between subspaces of a unitary space are characterized by a min-max property which involves inner products.},
author = {Rakocevic, Vladimir and Wimmer, Harald K.},
doi = {10.1007/s00022-003-1687-x},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rakocevic, Wimmer - 2003 - A variational characterization of canonical angles between subspaces.pdf:pdf},
issn = {0047-2468},
journal = {Journal of Geometry},
keywords = {Mathematics and Statistics},
month = dec,
number = {1-2},
pages = {122--124},
publisher = {Birkh\"{a}user Basel},
title = {{A variational characterization of canonical angles between subspaces}},
url = {http://www.springerlink.com/content/u7ejruq24n3e52vd/},
volume = {78},
year = {2003}
}
@techreport{Kuss2003,
abstract = {Canonical correlation analysis (CCA) is a classical multivariate method concerned with describing linear dependencies between sets of variables. After a short exposition of the linear sample CCA problem and its analytical solution, the article proceeds with a detailed characterization of its geometry. Projection operators are used to illustrate the relations between canonical vectors and variates. The article then addresses the problem of CCA between spaces spanned by objects mapped into kernel feature spaces. An exact solution for this kernel canonical correlation (KCCA) problem is derived from a geometric point of view. It shows that the expansion coefficients of the canonical vectors in their respective feature space can be found by linear CCA in the basis induced by kernel principal component analysis. The effect of mappings into higher dimensional feature spaces is considered critically since it simplifies the CCA problem in general. Then two regularized variants of KCCA are discussed. Relations to other methods are illustrated, e.g., multicategory kernel Fisher discriminant analysis, kernel principal component regression and possible applications thereof in blind source separation.},
author = {Kuss, Malte and Graepel, Thore},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuss, Graepel - 2003 - The geometry of kernel canonical correlation analysis.pdf:pdf},
institution = {Citeseer},
title = {{The geometry of kernel canonical correlation analysis}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.5864\&amp;rep=rep1\&amp;type=pdf},
year = {2003}
}
@article{Trussell2005a,
author = {Trussell, H.J.},
doi = {10.1109/MLSP.2005.1532880},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trussell - 2005 - Dimensionality Reduction using a Mixed Norm Penalty Function.pdf:pdf},
isbn = {0-7803-9517-4},
journal = {2005 IEEE Workshop on Machine Learning for Signal Processing},
pages = {87--92},
publisher = {Ieee},
title = {{Dimensionality Reduction using a Mixed Norm Penalty Function}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1532880},
year = {2005}
}
@article{Davis1978a,
abstract = {Without Abstract},
author = {Davis, A. W.},
doi = {10.1007/BF02480202},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis - 1978 - On the asymptotic distribution of Gower'sm 2 goodness-of-fit criterion in a particular case.pdf:pdf},
issn = {0020-3157},
journal = {Annals of the Institute of Statistical Mathematics},
month = dec,
number = {1},
pages = {71--79},
title = {{On the asymptotic distribution of Gower'sm 2 goodness-of-fit criterion in a particular case}},
url = {http://www.springerlink.com/content/n0h557316148t1w4},
volume = {30},
year = {1978}
}
@article{Choi2008a,
abstract = {Most manifold learning methods consider only one similarity matrix to induce a low-dimensional manifold embedded in data space. In practice, however, we often use multiple sensors at a time so that each sensory information yields different similarity matrix derived from the same objects. In such a case, manifold integration is a desirable task, combining these similarity matrices into a compromise matrix that faithfully reflects multiple sensory information. A small number of methods exists for manifold integration, including a method based on reproducing kernel Krein space (RKKS) or DISTATIS, where the former is restricted to the case of only two manifolds and the latter considers a linear combination of normalized similarity matrices as a compromise matrix. In this paper we present a new manifold integration method, Markov random walk on multiple manifolds (RAMS), which integrates transition probabilities defined on each manifold to compute a compromise matrix. Numerical experiments confirm that RAMS finds more informative manifolds with a desirable projection property.},
author = {Choi, Heeyoul and Choi, Seungjin and Choe, Yoonsuck},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Choi, Choe - 2008 - Manifold integration with Markov random walks.pdf:pdf},
journal = {Aaai Conference On Artificial Intelligence},
pages = {424--429},
title = {{Manifold integration with Markov random walks}},
url = {http://portal.acm.org/citation.cfm?id=1620064},
year = {2008}
}
@article{Ramsay1982,
author = {Ramsay, J. O.},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ramsay - 1982 - Some Statistical Approaches to Multidimensional Scaling Data.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series A (General)},
number = {3},
pages = {285 -- 312},
title = {{Some Statistical Approaches to Multidimensional Scaling Data}},
url = {http://www.jstor.org/stable/2981865},
volume = {145},
year = {1982}
}
@article{Ham2004,
abstract = {We interpret several well-known algorithms for dimensionality reduction of manifolds as kernel methods. Isomap, graph Laplacian eigenmap, and locally linear embedding (LLE) all utilize local neighborhood information to construct a global embedding of the manifold. We show how all three algorithms can be described as kernel PCA on specially constructed Gram matrices, and illustrate the similarities and differences between the algorithms with representative examples.},
author = {Ham, Jihun and Lee, Daniel D. and Mika, Sebastian and Sch\"{o}lkopf, Bernhard},
journal = {ACM International Conference Proceeding Series; Vol. 69},
title = {{A kernel view of the dimensionality reduction of manifolds}},
url = {http://portal.acm.org/citation.cfm?id=1015417},
year = {2004}
}
@article{Qiu2006,
author = {Qiu, L and Zhang, Y and Li, CK},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qiu, Zhang, Li - 2006 - Unitarily invariant metrics on the Grassmann space.pdf:pdf},
issn = {0895-4798},
journal = {SIAM journal on matrix analysis and applications},
number = {2},
pages = {507--531},
publisher = {Citeseer},
title = {{Unitarily invariant metrics on the Grassmann space}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.4881\&amp;rep=rep1\&amp;type=pdf},
volume = {27},
year = {2006}
}
@article{Xia2010,
abstract = {In computer vision and multimedia search, it is common to use multiple features from different views to represent an object. For example, to well characterize a natural scene image, it is essential to find a set of visual features to represent its color, texture, and shape information and encode each feature into a vector. Therefore, we have a set of vectors in different spaces to represent the image. Conventional spectral-embedding algorithms cannot deal with such datum directly, so we have to concatenate these vectors together as a new vector. This concatenation is not physically meaningful because each feature has a specific statistical property. Therefore, we develop a new spectral-embedding algorithm, namely, multiview spectral embedding (MSE), which can encode different features in different ways, to achieve a physically meaningful embedding. In particular, MSE finds a low-dimensional embedding wherein the distribution of each view is sufficiently smooth, and MSE explores the complementary property of different views. Because there is no closed-form solution for MSE, we derive an alternating optimization-based iterative algorithm to obtain the low-dimensional embedding. Empirical evaluations based on the applications of image retrieval, video annotation, and document clustering demonstrate the effectiveness of the proposed approach.},
author = {Xia, Tian and Tao, Dacheng and Mei, Tao and Zhang, Yongdong},
doi = {10.1109/TSMCB.2009.2039566},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xia et al. - 2010 - Multiview spectral embedding.pdf:pdf},
issn = {1941-0492},
journal = {IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society},
month = dec,
number = {6},
pages = {1438--46},
pmid = {20172832},
title = {{Multiview spectral embedding.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20172832},
volume = {40},
year = {2010}
}
@inproceedings{Ham2005a,
abstract = {In this paper, we study a family of semisupervised learning algorithms for "aligning" di\#erent data sets that are characterized by the same underlying manifold. The optimizations of these algorithms are based on graphs that provide a discretized approximation to the manifold. Partial alignments of the data sets---obtained from prior knowledge of their manifold structure or from pairwise correspondences of subsets of labeled examples--- are completed by integrating supervised signals with unsupervised frameworks for manifold learning. As an illustration of this semisupervised setting, we show how to learn mappings between di\#erent data sets of images that are parameterized by the same underlying modes of variability (e.g., pose and viewing angle). The curse of dimensionality in these problems is overcome by exploiting the low dimensional structure of image manifolds.},
author = {Ham, Jihun and Lee, D and Saul, L.},
booktitle = {Proceedings of the Annual Conference on Uncertainty in Artificial Intelligence, Z. Ghahramani and R. Cowell, Eds},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ham, Lee, Saul - 2005 - Semisupervised alignment of manifolds.pdf:pdf},
pages = {120--127},
publisher = {Citeseer},
title = {{Semisupervised alignment of manifolds}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.59.8098\&amp;rep=rep1\&amp;type=pdf},
volume = {10},
year = {2005}
}
@article{Krafft,
author = {Krafft, Peter and Mahadevan, S.},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krafft, Mahadevan - Unknown - Feature-Preserving Embeddings for Topic Transfer.pdf:pdf},
journal = {cs.umass.edu},
pages = {1--4},
title = {{Feature-Preserving Embeddings for Topic Transfer}},
url = {http://www.cs.umass.edu/~pkrafft/research/2010KrafftMahadevanFeaturePreservingEmbeddings.pdf}
}
@inproceedings{Zhai2010,
author = {Zhai, D. and Li, B. and Chang, H. and Shan, S. and Chen, X. and Gao, W. and CAS, C.},
booktitle = {Proceedings of the British Machine Vision Conference},
doi = {10.5244/C.24.3},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhai et al. - 2010 - Manifold Alignment via Corresponding Projections.pdf:pdf},
isbn = {1-901725-40-5},
pages = {3.1--3.11},
publisher = {British Machine Vision Association},
title = {{Manifold Alignment via Corresponding Projections}},
url = {http://www.jdl.ac.cn/doc/2010/DemingZhai\_BMVC\_2010.pdf},
year = {2010}
}
@article{Prince2010,
author = {Prince, Simon J.D. and Elder, James H.},
doi = {10.1109/CRV.2010.12},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Prince, Elder - 2010 - Bayesian Identity Clustering.pdf:pdf},
isbn = {978-1-4244-6963-5},
journal = {2010 Canadian Conference on Computer and Robot Vision},
pages = {32--39},
publisher = {Ieee},
title = {{Bayesian Identity Clustering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5479489},
year = {2010}
}
@article{Marchette2008b,
author = {Marchette, D and Priebe, C},
doi = {10.1016/j.csda.2007.03.016},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marchette, Priebe - 2008 - Predicting unobserved links in incompletely observed networks.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics \& Data Analysis},
keywords = {covert networks,dot product graphs,interstate alliances,link prediction,random graphs,social networks},
month = jan,
number = {3},
pages = {1373--1386},
title = {{Predicting unobserved links in incompletely observed networks}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947307001193},
volume = {52},
year = {2008}
}
@article{Geyer2009,
author = {Geyer, Charles J},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Geyer - 2009 - MCMC Package Example ( Version 0 . 7-3 ).pdf:pdf},
journal = {Time},
title = {{MCMC Package Example ( Version 0 . 7-3 )}},
volume = {1},
year = {2009}
}
@article{Package2010,
author = {Package, Type and Analytics, Author Revolution},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Package, Analytics - 2010 - Package ‘ doMC ’.pdf:pdf},
pages = {1--4},
title = {{Package ‘ doMC ’}},
year = {2010}
}
@inproceedings{El-arini2008,
author = {El-arini, Khalid},
booktitle = {October},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/El-arini - 2008 - Dirichlet Processes A gentle tutorial.pdf:pdf},
title = {{Dirichlet Processes A gentle tutorial}},
year = {2008}
}
@article{Woodard2009,
author = {Woodard, D. and Goldszmidt, M},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Woodard, Goldszmidt - 2009 - Model-based clustering for online crisis identification in distributed computing.pdf:pdf},
journal = {Distributed computing},
pages = {1--21},
title = {{Model-based clustering for online crisis identification in distributed computing}},
url = {http://research.microsoft.com/pubs/102706/ehsStats (4).pdf},
year = {2009}
}
@article{Reckdahl2006,
author = {Reckdahl, Keith},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reckdahl - 2006 - A T X and pdfL Using Imported Graphics in L.pdf:pdf},
journal = {Reproduction},
title = {{A T X and pdfL Using Imported Graphics in L}},
year = {2006}
}
@article{Geman1982,
author = {Geman, S and Hwang, CR},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Geman, Hwang - 1982 - Nonparametric maximum likelihood estimation by the method of sieves.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {401----414},
title = {{Nonparametric maximum likelihood estimation by the method of sieves}},
url = {http://www.jstor.org/stable/2240675},
volume = {10},
year = {1982}
}
@article{,
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Filter and Identify Peaks.pdf:pdf},
title = {{Filter and Identify Peaks}}
}
@article{Wang2007,
author = {Wang, Xin and Kab\'{a}n, Ata},
doi = {10.1007/s10618-007-0081-y},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Kab\'{a}n - 2007 - A dynamic bibliometric model for identifying online communities.pdf:pdf},
issn = {1384-5810},
journal = {Data Mining and Knowledge Discovery},
month = aug,
number = {1},
pages = {67--107},
title = {{A dynamic bibliometric model for identifying online communities}},
url = {http://www.springerlink.com/index/10.1007/s10618-007-0081-y},
volume = {16},
year = {2007}
}
@article{Smith2009,
author = {Smith, Toby and Alahakoon, Damminda},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smith, Alahakoon - 2009 - Growing Self-Organizing Map for Online Continuous Clustering.pdf:pdf},
journal = {Foundations of Computational Intelligence Volume 4},
pages = {49--83},
publisher = {Springer},
title = {{Growing Self-Organizing Map for Online Continuous Clustering}},
url = {http://www.springerlink.com/index/Y3G80271G1147611.pdf},
volume = {4},
year = {2009}
}
@inproceedings{Shamir2008,
author = {Shamir, Ohad and Tishby, Naftali},
booktitle = {Proceedings of the 21rst Annual Conference on Learning Theory (COLT)},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shamir, Tishby - 2008 - Model selection and stability in k-means clustering.pdf:pdf},
publisher = {Citeseer},
title = {{Model selection and stability in k-means clustering}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.5037\&amp;rep=rep1\&amp;type=pdf},
year = {2008}
}
@article{Weston2010a,
author = {Weston, Steve},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weston - 2010 - Getting Started with doMC and foreach.pdf:pdf},
pages = {1--6},
title = {{Getting Started with doMC and foreach}},
year = {2010}
}
@article{Turner2007,
author = {Turner, W.J.},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Turner - 2007 - Basic LATEX Typesetting.pdf:pdf},
journal = {Computer},
title = {{Basic LATEX Typesetting}},
url = {http://persweb.wabash.edu/facstaff/turnerw/Writing/LaTeX/latex.pdf},
year = {2007}
}
@article{Genz2011,
author = {Genz, Author Alan and Bretz, Frank and Miwa, Tetsuhisa and Mi, Xuefei and Scheipl, Fabian and Bornkamp, Bjoern and Hothorn, Torsten},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Genz et al. - 2011 - Package ‘ mvtnorm ’.pdf:pdf},
journal = {Matrix},
title = {{Package ‘ mvtnorm ’}},
year = {2011}
}
@article{Zhang2005a,
author = {Zhang, Jian and Ghahramani, Zoubin},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Ghahramani - 2005 - A probabilistic model for online document clustering with application to novelty detection.pdf:pdf},
journal = {English},
title = {{A probabilistic model for online document clustering with application to novelty detection}},
url = {http://eprints.pascal-network.org/archive/00000771/},
year = {2005}
}
@article{Vidal2007,
author = {Vidal, R.},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vidal - 2007 - Online clustering of moving hyperplanes.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1433},
publisher = {MIT; 1998},
title = {{Online clustering of moving hyperplanes}},
url = {http://cognet.mit.edu/library/books/mitpress/0262195682/cache/chap180.pdf},
volume = {19},
year = {2007}
}
@article{Kandylas2009,
author = {Kandylas, Vasileios},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kandylas - 2009 - Online clustering and citation analysis using Streemer.pdf:pdf},
journal = {Publicly accessible Penn Dissertations},
title = {{Online clustering and citation analysis using Streemer}},
url = {http://repository.upenn.edu/cgi/viewcontent.cgi?article=1052\&amp;context=edissertations},
year = {2009}
}
@article{McCallum2000,
address = {New York, New York, USA},
author = {McCallum, Andrew and Nigam, Kamal and Ungar, Lyle H.},
doi = {10.1145/347090.347123},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McCallum, Nigam, Ungar - 2000 - Efficient clustering of high-dimensional data sets with application to reference matching.pdf:pdf},
isbn = {1581132336},
journal = {Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '00},
pages = {169--178},
publisher = {ACM Press},
title = {{Efficient clustering of high-dimensional data sets with application to reference matching}},
url = {http://portal.acm.org/citation.cfm?doid=347090.347123},
year = {2000}
}
@inproceedings{Das2007,
author = {Das, A.S. and Datar, M. and Garg, A. and Rajaram, S.},
booktitle = {Proceedings of the 16th international conference on World Wide Web},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Das et al. - 2007 - Google news personalization scalable online collaborative filtering.pdf:pdf},
keywords = {google news,mapreduce,mendation system,minhash,online recom-,plsi,scalable collaborative filtering},
pages = {271--280},
publisher = {ACM},
title = {{Google news personalization: scalable online collaborative filtering}},
url = {http://dl.acm.org/citation.cfm?id=1242610)},
year = {2007}
}
@article{Pham2005,
author = {Pham, D T and Dimov, S S and Nguyen, C D},
doi = {10.1243/095440605X8298},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pham, Dimov, Nguyen - 2005 - Selection of iKi in iKi-means clustering.pdf:pdf},
issn = {0954-4062},
journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
keywords = {cluster number selection,clustering,k-means algorithm},
month = apr,
number = {1},
pages = {103--119},
title = {{Selection of <i>K</i> in <i>K</i>-means clustering}},
url = {http://journals.pepublishing.com/openurl.asp?genre=article\&id=doi:10.1243/095440605X8298},
volume = {219},
year = {2005}
}
@article{Symbols2005,
author = {Symbols, Mathematical},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Symbols - 2005 - Lyx 1 . 3 Keyboard Shortcuts.pdf:pdf},
number = {December},
pages = {1--2},
title = {{Lyx 1 . 3 : Keyboard Shortcuts}},
year = {2005}
}
@article{Parallel2010,
author = {Parallel, Scalable and Number, Random and Na, Author and Na, Maintainer and Provides, Description and Gpl, License and Date, Repository Cran},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parallel et al. - 2010 - Package ‘ rsprng ’.pdf:pdf},
journal = {Library},
title = {{Package ‘ rsprng ’}},
year = {2010}
}
@inproceedings{Rasanen2009,
author = {R\"{a}s\"{a}nen, O.J. and Laine, U.K. and Altosaar, Toomas},
booktitle = {Tenth Annual Conference of the International Speech Communication Association},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/R\"{a}s\"{a}nen, Laine, Altosaar - 2009 - Self-learning vector quantization for pattern discovery from speech.pdf:pdf},
pages = {1--4},
title = {{Self-learning vector quantization for pattern discovery from speech}},
url = {http://www.isca-speech.org/archive/interspeech\_2009/i09\_0852.html},
year = {2009}
}
@article{Romberg2007,
author = {Romberg, Justin},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Romberg - 2007 - Compressed sensing A tutorial.pdf:pdf},
journal = {IEEE Work. Stat. Signal Processing (SSP)},
title = {{Compressed sensing: A tutorial}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Compressed+Sensing:+A+Tutorial\#4},
year = {2007}
}
@article{Agrawal2005,
author = {Agrawal, Rakesh and Gehrke, Johannes and Gunopulos, Dimitrios and Raghavan, Prabhakar},
doi = {10.1007/s10618-005-1396-1},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal et al. - 2005 - Automatic Subspace Clustering of High Dimensional Data.pdf:pdf},
issn = {1384-5810},
journal = {Data Mining and Knowledge Discovery},
keywords = {clustering,dimensionality reduction,subspace clustering},
month = jul,
number = {1},
pages = {5--33},
title = {{Automatic Subspace Clustering of High Dimensional Data}},
url = {http://www.springerlink.com/index/10.1007/s10618-005-1396-1},
volume = {11},
year = {2005}
}
@phdthesis{Wang2010,
author = {Wang, Chang},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang - 2010 - A geometric framework for transfer learning using manifold alignment.pdf:pdf},
keywords = {Manifold Alignment,Representat,Transfer Learning},
number = {September},
publisher = {Citeseer},
title = {{A geometric framework for transfer learning using manifold alignment}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.6983\&amp;rep=rep1\&amp;type=pdf},
year = {2010}
}
@inproceedings{Trosset2006,
author = {Trosset, Michael W},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trosset - 2006 - Multidimensional Scaling Algorithms for Small and Large Data Sets.pdf:pdf},
number = {April},
title = {{Multidimensional Scaling Algorithms for Small and Large Data Sets}},
year = {2006}
}
@inproceedings{Verbeek2004,
author = {Verbeek, J.J. and Roweis, S.T. and Vlassis, Nikos},
booktitle = {Advances in neural information processing systems 16: proceedings of the 2003 conference},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Verbeek, Roweis, Vlassis - 2004 - Non-linear CCA and PCA by alignment of local models.pdf:pdf},
pages = {297},
publisher = {The MIT Press},
title = {{Non-linear CCA and PCA by alignment of local models}},
url = {http://books.google.com/books?hl=en\&amp;lr=\&amp;id=0F-9C7K8fQ8C\&amp;oi=fnd\&amp;pg=PA297\&amp;dq=Non-linear+CCA+and+PCA+by+Alignment+of+Local+Models\&amp;ots=TGExjTQe7Z\&amp;sig=frZGqP2GAOAnaRM4YnP00g\_7yGA},
volume = {16},
year = {2004}
}
@article{Acito1980,
author = {Acito, F. and Anderson, R.D. and Engledow, J.L.},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Acito, Anderson, Engledow - 1980 - A Simulation Study of Methods for Hypothesis Testing in Factor Analysis.pdf:pdf},
journal = {Journal of Consumer Research},
number = {2},
pages = {141--150},
title = {{A Simulation Study of Methods for Hypothesis Testing in Factor Analysis}},
url = {http://www.jstor.org/stable/2489081},
volume = {7},
year = {1980}
}
@article{Hand2006b,
abstract = {A great many tools have been developed for supervised classification, ranging from early methods such as linear discriminant analysis through to modern developments such as neural networks and support vector machines. A large number of comparative studies have been conducted in attempts to establish the relative superiority of these methods. This paper argues that these comparisons often fail to take into account important aspects of real problems, so that the apparent superiority of more sophisticated methods may be something of an illusion. In particular, simple methods typically yield performance almost as good as more sophisticated methods, to the extent that the difference in performance may be swamped by other sources of uncertainty that generally are not considered in the classical supervised classification paradigm.},
archivePrefix = {arXiv},
arxivId = {math/0606441},
author = {Hand, David J.},
doi = {10.1214/088342306000000060},
eprint = {0606441},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hand - 2006 - Classifier Technology and the Illusion of Progress.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Statistics,Theory,and phrases,empirical comparisons,error rate,flat maximum effect,misclassifica-,misclassification rate,population drift,principle of parsimony,problem uncertainty,selectivity bias,simplicity,supervised classification,tion rate},
month = feb,
number = {1},
pages = {1--14},
primaryClass = {math},
title = {{Classifier Technology and the Illusion of Progress}},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.ss/1149600839/ http://arxiv.org/abs/math/0606441 http://projecteuclid.org/euclid.ss/1149600839},
volume = {21},
year = {2006}
}
@article{Pan2009a,
annote = {From Duplicate 1 ( },
author = {Pan, Sinno Jialin and Fellow, Qiang Yang},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - A Survey on Transfer Learning.html:html;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan, Fellow - 2009 - A Survey on Transfer Learning.pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data Engineering},
pages = {1--15},
title = {{A Survey on Transfer Learning}},
url = {http://www.cse.ust.hk/~sinnopan/publications/TLsurvey\_0822.pdf},
year = {2009}
}
@article{Ben-David2006,
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, J.W.},
doi = {10.1007/s10994-009-5152-4},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben-David et al. - 2009 - A theory of learning from different domains.pdf:pdf;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben-David et al. - 2009 - A theory of learning from different domains(2).pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
number = {1-2},
pages = {151--175},
publisher = {Springer},
title = {{A theory of learning from different domains}},
url = {http://www.springerlink.com/index/q6qk230685577n52.pdf},
volume = {79},
year = {2009}
}
@article{Belkin2003,
author = {Belkin, Mikhail and Niyogi, Partha},
doi = {10.1162/089976603321780317},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Belkin, Niyogi - 2003 - Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = jun,
number = {6},
pages = {1373--1396},
publisher = {MIT Press},
title = {{Laplacian Eigenmaps for Dimensionality Reduction and Data Representation}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976603321780317},
volume = {15},
year = {2003}
}
@article{Sibson,
author = {Sibson, Robin},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sibson - 1979 - Studies in the Robustness of Multidimensional Scaling Perturbational Analysis of Classical Scaling.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
keywords = {multidimensional scaling,procrustes analysis,robustness,rotational fit},
number = {2},
pages = {217--229},
title = {{Studies in the Robustness of Multidimensional Scaling: Perturbational Analysis of Classical Scaling}},
volume = {40},
year = {1979}
}
@article{Zadrozny2004,
abstract = {Classifier learning methods commonly assume that the training data consist of randomly drawn examples from the same distribution as the test examples about which the learned model is expected to make predictions. In many practical situations, however, this assumption is violated, in a problem known in econometrics as sample selection bias. In this paper, we formalize the sample selection bias problem in machine learning terms and study analytically and experimentally how a number of well-known classifier learning methods are affected by it. We also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias.},
address = {New York, New York, USA},
author = {Zadrozny, Bianca},
doi = {10.1145/1015330.1015425},
isbn = {1581138285},
journal = {ACM International Conference Proceeding Series; Vol. 69},
pages = {114},
publisher = {ACM Press},
title = {{Learning and evaluating classifiers under sample selection bias}},
url = {http://portal.acm.org/citation.cfm?id=1015425 http://portal.acm.org/citation.cfm?doid=1015330.1015425},
year = {2004}
}
@article{Langrona,
author = {Langron, S. P. SP and Collins, AJ J.},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Langron, Collins - Unknown - Perturbation Theory for Generalized Procrustes Analysis.pdf:pdf},
issn = {0035-9246},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
keywords = {generalized procrustes analysis,perturbation},
number = {2},
pages = {277--284},
publisher = {JSTOR},
title = {{Perturbation Theory for Generalized Procrustes Analysis}},
url = {http://www.jstor.org/stable/2345571},
volume = {47},
year = {1985}
}
@article{Weston2010,
author = {Weston, Jason and Bengio, Samy and Usunier, Nicolas},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weston, Bengio, Usunier - 2010 - Large scale image annotation learning to rank with joint word-image embeddings.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
pages = {1--15},
publisher = {Springer},
title = {{Large scale image annotation: learning to rank with joint word-image embeddings}},
url = {http://www.springerlink.com/index/Y277128518468756.pdf},
year = {2010}
}
@article{Saul2001,
author = {Saul, Lawrence K L.K. and Ave, Park and Park, Florham and Roweis, S.T. Sam T},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saul, Roweis - 2001 - An introduction to locally linear embedding.pdf:pdf},
journal = {Available from иид ЛЛлллК зКигжгвигК йЛ жгл зЛаа Л},
publisher = {Citeseer},
title = {{An introduction to locally linear embedding}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.123.7319\&amp;rep=rep1\&amp;type=pdf},
year = {2001}
}
@book{Shaw2009,
address = {New York, New York, USA},
author = {Shaw, Blake and Jebara, Tony},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
doi = {10.1145/1553374.1553494},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shaw, Jebara - 2009 - Structure preserving embedding.pdf:pdf},
isbn = {9781605585161},
month = jun,
pages = {1--8},
publisher = {ACM Press},
title = {{Structure preserving embedding}},
url = {http://portal.acm.org/citation.cfm?id=1553374.1553494},
year = {2009}
}
@article{Guha2003a,
author = {Guha, S. and Meyerson, a. and Mishra, N. and Motwani, R. and O'Callaghan, L.},
doi = {10.1109/TKDE.2003.1198387},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guha et al. - 2003 - Clustering data streams theory and practice.pdf:pdf;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guha et al. - 2003 - Clustering data streams theory and practice(2).pdf:pdf},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
month = may,
number = {3},
pages = {515--528},
title = {{Clustering data streams: theory and practice}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1198387},
volume = {15},
year = {2003}
}
@article{Shteynberg,
author = {Shteynberg, David and Ms, Lc-ms},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shteynberg, Ms - Unknown - iTRAQ Quantitation with Libra Peptide and Protein Quantitation.pdf:pdf},
pages = {1--14},
title = {{iTRAQ Quantitation with Libra Peptide and Protein Quantitation}}
}
@inproceedings{Babcock2002,
author = {Babcock, B. and Datar, M. and Motwani, R.},
booktitle = {Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Babcock, Datar, Motwani - 2002 - Sampling from a moving window over streaming data.pdf:pdf},
pages = {633--634},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Sampling from a moving window over streaming data}},
url = {http://portal.acm.org/citation.cfm?id=545465},
year = {2002}
}
@article{Ravi2007,
author = {Ravi, V. and Srinivas, E. R. and Kasabov, N. K.},
doi = {10.1109/ICCIMA.2007.111},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ravi, Srinivas, Kasabov - 2007 - On-Line Evolving Fuzzy Clustering.pdf:pdf},
isbn = {0-7695-3050-8},
journal = {International Conference on Computational Intelligence and Multimedia Applications (ICCIMA 2007)},
month = dec,
pages = {347--351},
publisher = {Ieee},
title = {{On-Line Evolving Fuzzy Clustering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4426605},
year = {2007}
}
@article{Chen2009,
author = {Chen, Hung-leng and Chen, Ming-syan and Lin, Su-chen},
doi = {10.1109/TKDE.2008.192},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Chen, Lin - 2009 - Catching the Trend A Framework for Clustering Concept-Drifting Categorical Data.pdf:pdf},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
month = may,
number = {5},
pages = {652--665},
title = {{Catching the Trend: A Framework for Clustering Concept-Drifting Categorical Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4626958},
volume = {21},
year = {2009}
}
@article{Vijayakumar2005,
abstract = {Locally weighted projection regression (LWPR) is a new algorithm for incremental nonlinear function approximation in high-dimensional spaces with redundant and irrelevant input dimensions. At its core, it employs nonparametric regression with locally linear models. In order to stay computationally efficient and numerically robust, each local model performs the regression analysis with a small number of univariate regressions in selected directions in input space in the spirit of partial least squares regression. We discuss when and how local learning techniques can successfully work in high-dimensional spaces and review the various techniques for local dimensionality reduction before finally deriving the LWPR algorithm. The properties of LWPR are that it (1) learns rapidly with second-order learning methods based on incremental training, (2) uses statistically sound stochastic leave-one-out cross validation for learning without the need to memorize training data, (3) adjusts its weighting kernels based on only local information in order to minimize the danger of negative interference of incremental learning, (4) has a computational complexity that is linear in the number of inputs, and (5) can deal with a large number of-possibly redundant-inputs, as shown in various empirical evaluations with up to 90 dimensional data sets. For a probabilistic interpretation, predictive variance and confidence intervals are derived. To our knowledge, LWPR is the first truly incremental spatially localized learning method that can successfully and efficiently operate in very high-dimensional spaces.},
author = {Vijayakumar, Sethu and D'Souza, Aaron and Schaal, Stefan},
doi = {10.1162/089976605774320557},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vijayakumar, D'Souza, Schaal - 2005 - Incremental online learning in high dimensions.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Learning,Models, Theoretical,Neural Networks (Computer),Nonlinear Dynamics,Pattern Recognition, Automated,Robotics},
month = dec,
number = {12},
pages = {2602--34},
pmid = {16212764},
title = {{Incremental online learning in high dimensions.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16212764},
volume = {17},
year = {2005}
}
@book{Beringer2003,
author = {Beringer, J and H\"{u}llermeier, E.},
booktitle = {Work},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Beringer, H\"{u}llermeier - 2003 - Online clustering of data streams.pdf:pdf},
pages = {1--12},
publisher = {Citeseer},
title = {{Online clustering of data streams}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.728\&amp;rep=rep1\&amp;type=pdf},
year = {2003}
}
@article{Neumann2010,
author = {Neumann, S and Kutzera, J},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neumann, Kutzera - 2010 - Processing Tandem-MS and MS n data with xcms.pdf:pdf;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neumann, Kutzera - 2011 - Processing Tandem-MS and MS n data with xcms.pdf:pdf},
journal = {October},
pages = {1--3},
title = {{Processing Tandem-MS and MS n data with xcms}},
year = {2010}
}
@article{Duan2011,
abstract = {Cross-domain learning methods have shown promising results by leveraging labeled patterns from the auxiliary domain to learn a robust classifier for the target domain which has only a limited number of labeled samples. To cope with the considerable change between feature distributions of different domains, we propose a new cross-domain kernel learning framework into which many existing kernel methods can be readily incorporated. Our framework, referred to as Domain Transfer Multiple Kernel Learning (DTMKL), simultaneously learns a kernel function and a robust classifier by minimizing both the structural risk functional and the distribution mismatch between labeled and unlabeled samples from the auxiliary and target domains. Under the DTMKL framework, we also propose two novel methods by using SVM and pre-learned classifiers, respectively. Comprehensive experiments on three domain adaptation data sets (i.e., TRECVID, 20 Newsgroups and email spam data sets) demonstrate that DTMKL based methods outperform existing cross-domain learning and multiple kernel learning methods.},
author = {Duan, Lixin and Tsang, I. and Xu, Dong},
doi = {10.1109/TPAMI.2011.114},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duan, Tsang, Xu - 2011 - Domain Transfer Multiple Kernel Learning.pdf:pdf},
issn = {1939-3539},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
month = may,
number = {99},
pages = {1--1},
pmid = {21646679},
publisher = {IEEE},
title = {{Domain Transfer Multiple Kernel Learning}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21646679 http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5871644},
year = {2011}
}
@article{Blanchard2011,
author = {Blanchard, Gilles and Lee, Gyemin},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blanchard, Lee - 2011 - Generalizing from several related classification tasks to a new unlabeled sample.pdf:pdf},
number = {i},
pages = {1--9},
title = {{Generalizing from several related classification tasks to a new unlabeled sample}},
url = {http://eprints.pascal-network.org/archive/00008698/},
volume = {1},
year = {2011}
}
@article{Sricharan,
author = {Sricharan, Kumar and Raich, Raviv},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sricharan, Raich - Unknown - PERFORMANCE-DRIVEN ENTROPIC INFORMATION FUSION.pdf:pdf},
journal = {eecs.umich.edu},
title = {{PERFORMANCE-DRIVEN ENTROPIC INFORMATION FUSION}},
url = {http://www.eecs.umich.edu/techreports/systems/cspl/cspl-409.pdf}
}
@article{Lafon2006a,
abstract = {Data fusion and multicue data matching are fundamental tasks of high-dimensional data analysis. In this paper, we apply the recently introduced diffusion framework to address these tasks. Our contribution is three-fold: First, we present the Laplace-Beltrami approach for computing density invariant embeddings which are essential for integrating different sources of data. Second, we describe a refinement of the Nystr\"{o}m extension algorithm called "geometric harmonics." We also explain how to use this tool for data assimilation. Finally, we introduce a multicue data matching scheme based on nonlinear spectral graphs alignment. The effectiveness of the presented schemes is validated by applying it to the problems of lipreading and image sequence alignment.},
author = {Lafon, St\'{e}phane and Keller, Yosi and Coifman, Ronald R},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Cluster Analysis,Databases, Factual,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Information Storage and Retrieval,Information Storage and Retrieval: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Subtraction Technique},
month = nov,
number = {11},
pages = {1784--97},
title = {{Data fusion and multicue data matching by diffusion maps.}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1704834},
volume = {28},
year = {2006}
}
@article{Todros2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1111.6308v2},
author = {Todros, Koby and III, Alfred O. Hero},
eprint = {arXiv:1111.6308v2},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Todros, III - 2011 - On Measure Transformed Canonical Correlation Analysis.pdf:pdf},
journal = {Arxiv preprint arXiv:1111.6308},
pages = {1--32},
title = {{On Measure Transformed Canonical Correlation Analysis}},
url = {http://arxiv.org/abs/1111.6308},
year = {2011}
}
@article{Fraikin2008,
author = {Fraikin, Catherine and Nesterov, Yurii and {Van Dooren}, Paul},
doi = {10.1137/050643878},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fraikin, Nesterov, Van Dooren - 2008 - Optimizing the Coupling Between Two Isometric Projections of Matrices.pdf:pdf},
issn = {08954798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {generalized numerical range,isometry,singular value decompostion,trace maximization},
month = feb,
number = {1},
pages = {324},
title = {{Optimizing the Coupling Between Two Isometric Projections of Matrices}},
url = {http://dl.acm.org/citation.cfm?id=1404637.1404651},
volume = {30},
year = {2008}
}
@article{Crammer2008,
annote = {From Duplicate 2 ( },
author = {Crammer, Koby and Kearns, Michael and Wortman, Jennifer},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crammer, Kearns, Wortman - 2007 - Learning from multiple sources.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {error bounds,multi-task learning},
pages = {1757--1774},
publisher = {JMLR. org},
title = {{Learning from multiple sources}},
url = {http://portal.acm.org/citation.cfm?id=1442790},
volume = {9},
year = {2008}
}
@article{Pan2008a,
abstract = {Transfer learning addresses the problem of how to utilize plenty of labeled data in a source domain to solve related but different problems in a target domain, even when the training and testing problems have different distributions or features. In this paper, we consider transfer learning via dimensionality reduction. To solve this problem, we learn a low-dimensional latent feature space where the distributions between the source domain data and the target domain data are the same or close to each other. Onto this latent feature space, we project the data in related domains where we can apply standard learning algorithms to train classification or regression models. Thus, the latent feature space can be treated as a bridge of transferring knowledge from the source domain to the target domain. The main contribution of our work is that we propose a new dimensionality reduction method to find a latent space, which minimizes the distance between distributions of the data in different domains in a latent space. The effectiveness of our approach to transfer learning is verified by experiments in two real world applications: indoor WiFi localization and binary text classification.},
annote = {From Duplicate 2 ( },
author = {Pan, Sinno Jialin and Kwok, James T. and Yang, Qiang},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan, Kwok, Yang - 2008 - Transfer learning via dimensionality reduction.pdf:pdf},
journal = {Aaai Conference On Artificial Intelligence},
title = {{Transfer learning via dimensionality reduction}},
url = {http://portal.acm.org/citation.cfm?id=1620163.1620177 http://portal.acm.org/citation.cfm?id=1620177 https://www.aaai.org/Papers/AAAI/2008/AAAI08-108.pdf},
year = {2008}
}
@inproceedings{Wang,
address = {New York, New York, USA},
annote = {From Duplicate 1 ( },
author = {Wang, Chang and Mahadevan, Sridhar},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390297},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Mahadevan - Unknown - Manifold Alignment without Correspondence.html:html;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Manifold Alignment using Procrustes Analysis.pdf:pdf},
isbn = {9781605582054},
keywords = {Adaptation,Machine Learning,Multi-task,Transfer,domain adaptation,manifold,manifold align-,manifold alignment,manifold learning,ment,procrustes,transfer learning},
mendeley-tags = {manifold,manifold alignment,procrustes,transfer learning},
pages = {1120--1127},
publisher = {ACM Press},
title = {{Manifold Alignment without Correspondence}},
url = {http://icml2008.cs.helsinki.fi/papers/229.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.7514\&amp;rep=rep1\&amp;type=pdf http://ijcai.org/papers09/Papers/IJCAI09-214.pdf http://portal.acm.org/citation.cfm?doid=1390156.1390297 http://www.aaai.org/ocs/index.php/IJCAI/IJCAI-09/paper/viewPDFInterstitial/446/977},
year = {2008}
}
@article{Ben-David2006,
annote = {From Duplicate 2 ( },
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, J.W. Jennifer Wortman},
doi = {10.1007/s10994-009-5152-4},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben-David et al. - 2009 - A theory of learning from different domains.pdf:pdf;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben-David et al. - 2009 - A theory of learning from different domains(2).pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
number = {1-2},
pages = {151--175},
publisher = {Springer},
title = {{A theory of learning from different domains}},
url = {http://www.springerlink.com/index/10.1007/s10994-009-5152-4 http://www.springerlink.com/index/q6qk230685577n52.pdf},
volume = {79},
year = {2009}
}
@article{DaumeIII2006,
author = {{Daum\'{e} III}, H. and Marcu, Daniel},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Daum\'{e} III, Marcu - 2006 - Domain adaptation for statistical classifiers.pdf:pdf},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
number = {1},
pages = {101--126},
publisher = {AI Access Foundation},
title = {{Domain adaptation for statistical classifiers}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Domain+Adaptation+for+Statistical+Classifiers\#0},
volume = {26},
year = {2006}
}
@article{Zhou2007,
abstract = {We consider spectral clustering and transductive inference for data with multiple views. A typical example is the web, which can be described by either the hyperlinks between web pages or the words occurring in web pages. When each view is represented as a graph, one may convexly combine the weight matrices or the discrete Laplacians for each graph, and then proceed with existing clustering or classification techniques. Such a solution might sound natural, but its underlying principle is not clear. Unlike this kind of methodology, we develop multiview spectral clustering via generalizing the normalized cut from a single view to multiple views. We further build multiview transductive inference on the basis of multiview spectral clustering. Our framework leads to a mixture of Markov chains defined on every graph. The experimental evaluation on real-world web classification demonstrates promising results that validate our method.},
address = {New York, New York, USA},
author = {Zhou, Dengyong and Burges, Christopher J. C.},
doi = {10.1145/1273496.1273642},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Burges - 2007 - Spectral clustering and transductive learning with multiple views.pdf:pdf},
isbn = {9781595937933},
journal = {ICML; Vol. 227},
month = jun,
pages = {1159},
publisher = {ACM Press},
title = {{Spectral clustering and transductive learning with multiple views}},
url = {http://dl.acm.org/citation.cfm?id=1273496.1273642 http://portal.acm.org/citation.cfm?id=1273642},
year = {2007}
}
@article{Xiong2007,
abstract = {. We study the problem of manifold alignment, which aims at “aligning” different data sets that share a similar intrinsic manifold provided some supervision. Unlike traditional methods that rely on pairwise correspondences between the two data sets, our method only needs some relative comparison information like “A is more similar to B than A is to C”. This method provides a more flexible way to acquire the prior knowledge for alignment, thus is able to handle situations where corresponding pairs are hard or impossible to identify. We optimize our objective based on the graphs that give discrete approximations of the manifold. Further, the problem is formulated as a semi-definite programming (SDP) problem which can readily be solved. Finally, experimental results are presented to show the effectiveness of our method. 1},
annote = {From Duplicate 1 ( },
author = {Xiong, Liang and Wang, Fei and Zhang, Changshui},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiong, Wang, Zhang - 2007 - Semi-definite Manifold Alignment.pdf:pdf;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiong, Wang, Zhang - 2007 - Semi-definite manifold alignment(2).pdf:pdf},
journal = {Machine Learning: ECML 2007},
pages = {773--781},
title = {{Semi-definite manifold alignment}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.89.4555 http://www.springerlink.com/index/q338x77086q83412.pdf},
year = {2007}
}
@article{Xiong,
abstract = {. We study the problem of manifold alignment, which aims at “aligning” different data sets that share a similar intrinsic manifold provided some supervision. Unlike traditional methods that rely on pairwise correspondences between the two data sets, our method only needs some relative comparison information like “A is more similar to B than A is to C”. This method provides a more flexible way to acquire the prior knowledge for alignment, thus is able to handle situations where corresponding pairs are hard or impossible to identify. We optimize our objective based on the graphs that give discrete approximations of the manifold. Further, the problem is formulated as a semi-definite programming (SDP) problem which can readily be solved. Finally, experimental results are presented to show the effectiveness of our method. 1},
annote = {From Duplicate 2 ( },
author = {Xiong, Liang and Wang, Fei and Zhang, Changshui},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiong, Wang, Zhang - 2007 - Semi-definite Manifold Alignment.pdf:pdf;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiong, Wang, Zhang - 2007 - Semi-definite manifold alignment(2).pdf:pdf;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiong, Wang, Zhang - 2007 - Semi-definite Manifold Alignment(3).pdf:pdf},
journal = {Machine Learning: ECML 2007},
pages = {773--781},
title = {{Semi-definite Manifold Alignment}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.89.4555 http://www.springerlink.com/index/q338x77086q83412.pdf},
year = {2007}
}
@article{Rangarajan1996,
abstract = {A Lagrangian relaxation network for graph matching is presented. The problem is formulated as follows: given graphs G and g, find a permutation matrix M that brings the two sets of vertices into correspondence. Permutation matrix constraints are formulated in the framework of deterministic annealing. Our approach is in the same spirit as a Lagrangian decomposition approach in that the row and column constraints are satisfied separately with a Lagrange multiplier used to equate the two "solutions". Due to the unavoidable symmetries in graph isomorphism (resulting in multiple global minima), we add a symmetry-breaking self-amplification term in order to obtain a permutation matrix. With the application of a fixpoint preserving algebraic transformation to both the distance measure and self-amplification terms, we obtain a Lagrangian relaxation network. The network performs minimization with respect to the Lagrange parameters and maximization with respect to the permutation matrix variables. Simulation results are shown on 100 node random graphs and for a wide range of connectivities.},
annote = {From Duplicate 2 ( },
author = {Rangarajan, Anand and Mjolsness, E D},
doi = {10.1109/72.548165},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rangarajan, Mjolsness - 1996 - A Lagrangian relaxation network for graph matching.pdf:pdf},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
keywords = {graph matching},
mendeley-tags = {graph matching},
month = jan,
number = {6},
pages = {1365--81},
pmid = {18263531},
title = {{A Lagrangian relaxation network for graph matching.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18263531 http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=548165},
volume = {7},
year = {1996}
}
@article{Weng2011,
abstract = {The success of query-by-concept, recently proposed to cater to video retrieval needs, depends on the accuracy of concept-based video indexing. Unfortunately, it remains a challenge to recognize the presence of concepts in a video segment or extract an objective linguistic description from it because of the semantic gap, that is, the lack of correspondence between low-level features and high-level interpretation. This paper studies three issues with the aim to reduce such a gap: (1) how to explore cues beyond low-level features, (2) how to combine diverse cues to improve performance, and (3) how to utilize the learned knowledge when applying it to a new domain. To solve these problems, we propose a framework that jointly exploits multiple cues across multiple video domains. First, a recursive algorithm is proposed to learn both inter-concept and inter-shot relationships from annotations. Second, all concept labels for all shots are simultaneously refined in a single fusion model. Additionally, unseen shots are assigned pseudo-labels according to their initial scores so that relationships can be learned. Integration of cues embedded within training and testing video sets accommodates domain change. Experiments on popular concept detection benchmarks show that our framework is effective, achieving significant improvement over popular baselines.},
author = {Weng, Ming-Fang and Chuang, Yung-Yu},
doi = {10.1109/TPAMI.2011.273},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = dec,
pmid = {22201049},
publisher = {Published by the IEEE Computer Society},
title = {{Cross-Domain Multi-Cue Fusion for Concept-Based Video Indexing.}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/TPAMI.2011.273 http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6112775},
volume = {34},
year = {2011}
}
@article{Tripathi2010a,
author = {Tripathi, Abhishek and Klami, Arto and Ore\v{s}i\v{c}, Matej and Kaski, Samuel},
doi = {10.1007/s10618-010-0205-7},
issn = {1384-5810},
journal = {Data Mining and Knowledge Discovery},
month = nov,
number = {2},
pages = {300--321},
title = {{Matching samples of multiple views}},
url = {http://www.springerlink.com/content/6u1142246174151l/ http://link.springer.com/10.1007/s10618-010-0205-7},
volume = {23},
year = {2010}
}
@article{Wachinger2012,
abstract = {We address the alignment of a group of images with simultaneous registration. Therefore, we provide further insights into a recently introduced framework for multivariate similarity measures, referred to as $\backslash$emph\{accumulated pair-wise estimates\} (APE), and derive efficient optimization methods for it. More specifically, we show a strict mathematical deduction of APE from a maximum-likelihood framework and establish a connection to the congealing framework. This is only possible after an extension of the congealing framework with neighborhood information. Moreover, we address the increased computational complexity of simultaneous registration by deriving efficient gradient-based optimization strategies for APE: Gauss-Newton and the efficient second-order minimization (ESM). We present next to SSD the usage of intrinsically non-squared similarity measures in this least-squares optimization framework. The fundamental assumption of ESM, the approximation of the perfectly aligned moving image through the fixed image, limits its application to mono-modal registration. We therefore incorporate recently proposed structural representations of images, which allow us to perform multi-modal registration with ESM. Finally, we evaluate the performance of the optimization strategies with respect to the similarity measures, leading to very good results for ESM. The extension to multi-modal registration is in this context very interesting because it offers further possibilities for evaluations, due to publicly available data sets with ground-truth alignment.},
author = {Wachinger, Christian and Navab, Nassir},
doi = {10.1109/TPAMI.2012.196},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = sep,
pages = {1--1},
publisher = {IEEE},
title = {{Simultaneous Registration of Multiple Images: Similarity Metrics and Efficient Optimization}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/TPAMI.2012.196},
year = {2012}
}
@article{Schultz,
author = {Schultz, J Michael and Liberman, Mark},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schultz, Liberman - Unknown - Cross-Lingual Topic Tracking using idf-Weighted Cosine Coefficient.pdf:pdf},
pages = {2--4},
title = {{Cross-Lingual Topic Tracking using idf-Weighted Cosine Coefficient}}
}
@misc{,
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Semantic similarity analysis of protein data assessment with biological features and issues.html:html},
keywords = {bioinformatics,dissimilarity,protein,proteomics},
mendeley-tags = {bioinformatics,dissimilarity,protein,proteomics},
title = {{Semantic similarity analysis of protein data: assessment with biological features and issues}},
url = {http://bib.oxfordjournals.org/cgi/content/long/13/5/569},
urldate = {2012-11-09}
}
@article{Chen2012a,
abstract = {We propose a framework for adapting text mining models that discovers low-rank shared concept space. Our major characteristic of this concept space is that it explicitly minimizes the distribution gap between the source domain with sufficient labeled data and the target domain with only unlabeled data, while at the same time it minimizes the empirical loss on the labeled data in the source domain. Our method is capable of conducting the domain adaptation task both in the original feature space as well as in the transformed Reproducing Kernel Hilbert Space (RKHS) using kernel tricks. Theoretical analysis guarantees that the error of our adaptation model can be bounded with respect to the embedded distribution gap and the empirical loss in the source domain. We have conducted extensive experiments on two common text mining problems, namely, document classification and information extraction to demonstrate the efficacy of our proposed framework.},
author = {Chen, Bo and Lam, Wai and Tsang, Ivor W and Wong, Tak-Lam},
doi = {99DE8B2F-A484-49FD-B73E-2F754EBB7F65},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2012 - Discovering Low-Rank Shared Concept Space for Adapting Text Mining Models.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = oct,
number = {1},
pmid = {23165006},
title = {{Discovering Low-Rank Shared Concept Space for Adapting Text Mining Models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23165006},
volume = {6},
year = {2012}
}
@inproceedings{Roy2012,
author = {Roy, SD and Mei, Tao and Zeng, Wenjun and Li, Shipeng},
booktitle = {\ldots international conference on Multimedia},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roy et al. - 2012 - Socialtransfer cross-domain transfer learning from social streams for media applications.pdf:pdf},
isbn = {9781450310895},
keywords = {cross-domain media retrieval,recommendation,transfer learning},
title = {{Socialtransfer: cross-domain transfer learning from social streams for media applications}},
url = {http://dl.acm.org/citation.cfm?id=2393437},
year = {2012}
}
@article{Fu2013,
abstract = {The rapid development of social media sharing has created a huge demand for automatic media classification and annotation techniques. Attribute learning has emerged as a promising paradigm for bridging the semantic gap and addressing data sparsity via transferring attribute knowledge in object recognition and relatively simple action classification. In this paper, we address the task of attribute learning for understanding multimedia data with sparse and incomplete labels. In particular we focus on videos of social group activities, which are particularly challenging and topical examples of this task because of their multi-modal content and complex and unstructured nature relative to the density of annotations. To solve this problem, we (1) introduce a concept of semi-latent attribute space, expressing user-defined and latent attributes in a unified framework, and (2) propose a novel scalable probabilistic topic model for learning multi-modal semi-latent attributes, which dramatically reduces requirements for an exhaustive accurate attribute ontology and expensive annotation effort. We show that our framework is able to exploit latent attributes to outperform contemporary approaches for addressing a variety of realistic multimedia sparse data learning tasks including: multi-task learning, learning with label noise, N-shot transfer learning and importantly zero-shot learning.},
annote = {semilatent attributest},
author = {Fu, Yanwei and Hospedales, Timothy M. and Xiang, Tao and Gong, Shaogang},
doi = {10.1109/TPAMI.2013.128},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu et al. - 2013 - Learning Multi-modal Latent Attributes.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Video analysis,Vision and Scene Understanding,latent,multimodal},
mendeley-tags = {latent,multimodal},
number = {99},
pages = {1--1},
shorttitle = {Pattern Analysis and Machine Intelligence, IEEE Tr},
title = {{Learning Multi-modal Latent Attributes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6552193},
volume = {PP},
year = {2013}
}
@article{Yeh2014,
abstract = {We present a novel domain adaptation approach for solving cross-domain pattern recognition problems, i.e., the data or features to be processed and recognized are collected from different domains of interest. Inspired by canonical correlation analysis (CCA), we utilize the derived correlation subspace as a joint representation for associating data across different domains, and we advance reduced kernel techniques for kernel CCA (KCCA) if nonlinear correlation subspace are desirable. Such techniques not only makes KCCA computationally more efficient, potential over-fitting problems can be alleviated as well. Instead of directly performing recognition in the derived CCA subspace (as prior CCA-based domain adaptation methods did), we advocate the exploitation of domain transfer ability in this subspace, in which each dimension has a unique capability in associating cross-domain data. In particular, we propose a novel support vector machine (SVM) with a correlation regularizer, named correlation-transfer SVM, which incorporates the domain adaptation ability into classifier design for cross-domain recognition. We show that our proposed domain adaptation and classification approach can be successfully applied to a variety of cross-domain recognition tasks such as cross-view action recognition, handwritten digit recognition with different features, and image-to-text or text-to-image classification. From our empirical results, we verify that our proposed method outperforms state-of-the-art domain adaptation approaches in terms of recognition performance.},
author = {Yeh, Yi-Ren and Huang, Chun-Hao and Wang, Yu-Chiang Frank},
doi = {10.1109/TIP.2014.2310992},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
month = may,
number = {5},
pages = {2009--2018},
shorttitle = {Image Processing, IEEE Transactions on},
title = {{Heterogeneous Domain Adaptation and Classification by Exploiting the Correlation Subspace}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6763039},
volume = {23},
year = {2014}
}
@article{Bordes2013,
author = {Bordes, Antoine and Usunier, Nicolas},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bordes, Usunier - 2013 - Translating embeddings for modeling multi-relational data.pdf:pdf},
journal = {Advances in Neural \ldots},
pages = {1--9},
title = {{Translating embeddings for modeling multi-relational data}},
url = {http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data},
year = {2013}
}
@inproceedings{Gress2014,
address = {New York, New York, USA},
author = {Gress, Aubrey and Davidson, Ian},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management - CIKM '14},
doi = {10.1145/2661829.2662030},
isbn = {9781450325981},
keywords = {dimensionality reduction,heterogeneous data,spectral methods},
month = nov,
pages = {1169--1178},
publisher = {ACM Press},
title = {{A Flexible Framework for Projecting Heterogeneous Data}},
url = {http://dl.acm.org/citation.cfm?id=2661829.2662030},
year = {2014}
}
@article{Zitnik2015,
abstract = {For most problems in science and engineering we can obtain data sets that describe the observed system from variousperspectives and record the behavior of its individual components. Heterogeneous data sets can be collectively mined by data fusion. Fusion can focus on a specific target relation and exploit directly associated data together with contextual data and data about system’s constraints. In the paper we describe a data fusion approach with penalized matrix tri-factorization (DFMF) that simultaneouslyfactorizes data matrices to reveal hidden associations. The approach can directly consider any data that can be expressed in a matrix, including those from feature-based representations, ontologies, associations and networks. We demonstrate the utility of DFMF for gene function prediction task with eleven different data sources and for prediction of pharmacologic actions by fusing six data sources. Our data fusion algorithm compares favorably to alternative data integration approaches and achieves higher accuracy than can be obtained from any single data source alone.},
author = {Zitnik, Marinka and Zupan, Blaz},
doi = {10.1109/TPAMI.2014.2343973},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Approximation methods,Convergence,Data fusion,Data integration,Data models,Diseases,Linear programming,Predictive models,bioinformatics,cheminformatics,data mining,intermediate data integration,matrix factorization},
month = jan,
number = {1},
pages = {41--53},
shorttitle = {Pattern Analysis and Machine Intelligence, IEEE Tr},
title = {{Data Fusion by Matrix Factorization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6867358},
volume = {37},
year = {2015}
}
