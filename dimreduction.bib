@inproceedings{Wang,
address = {New York, New York, USA},
annote = {From Duplicate 1 ( },
author = {Wang, Chang and Mahadevan, Sridhar},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390297},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Mahadevan - Unknown - Manifold Alignment without Correspondence.html:html;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Manifold Alignment using Procrustes Analysis.pdf:pdf},
isbn = {9781605582054},
keywords = {Adaptation,Machine Learning,Multi-task,Transfer,domain adaptation,manifold,manifold align-,manifold alignment,manifold learning,ment,procrustes,transfer learning},
mendeley-tags = {manifold,manifold alignment,procrustes,transfer learning},
pages = {1120--1127},
publisher = {ACM Press},
title = {{Manifold Alignment without Correspondence}},
url = {http://icml2008.cs.helsinki.fi/papers/229.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.7514\&amp;rep=rep1\&amp;type=pdf http://ijcai.org/papers09/Papers/IJCAI09-214.pdf http://portal.acm.org/citation.cfm?doid=1390156.1390297 http://www.aaai.org/ocs/index.php/IJCAI/IJCAI-09/paper/viewPDFInterstitial/446/977},
year = {2008}
}
@misc{,
annote = {undefined},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Robust Distance Metric Learning with Auxiliary Knowledge.pdf:pdf},
keywords = {distance metric,transfer learning},
mendeley-tags = {distance metric,transfer learning},
title = {{Robust Distance Metric Learning with Auxiliary Knowledge}},
url = {http://ijcai.org/papers09/Papers/IJCAI09-223.pdf}
}
@article{Ling2008,
abstract = {Traditional spectral classification has been proved to be effective in dealing with both labeled and unlabeled data when these data are from the same domain. In many real world applications, however, we wish to make use of the labeled data from one domain (called in-domain) to classify the unlabeled data in a different domain (out-of-domain). This problem often happens when obtaining labeled data in one domain is difficult while there are plenty of labeled data from a related but different domain. In general, this is a transfer learning problem where we wish to classify the unlabeled data through the labeled data even though these data are not from the same domain. In this paper, we formulate this domain-transfer learning problem under a novel spectral classification framework, where the objective function is introduced to seek consistency between the in-domain supervision and the out-of-domain intrinsic structure. Through optimization of the cost function, the label information from the in-domain data is effectively transferred to help classify the unlabeled data from the out-of-domain. We conduct extensive experiments to evaluate our method and show that our algorithm achieves significant improvements on classification performance over many state-of-the-art algorithms.},
author = {Ling, Xiao and Dai, Wenyuan and Xue, Gui-Rong and Yang, Qiang and Yu, Yong},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ling et al. - 2008 - Spectral domain-transfer learning.pdf:pdf},
journal = {International Conference on Knowledge Discovery and Data Mining},
keywords = {spectral learning,transfer learning},
pages = {488--496},
title = {{Spectral domain-transfer learning}},
url = {http://portal.acm.org/citation.cfm?id=1401951},
year = {2008}
}
@article{Trussell2005a,
author = {Trussell, H.J.},
doi = {10.1109/MLSP.2005.1532880},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trussell - 2005 - Dimensionality Reduction using a Mixed Norm Penalty Function.pdf:pdf},
isbn = {0-7803-9517-4},
journal = {2005 IEEE Workshop on Machine Learning for Signal Processing},
pages = {87--92},
publisher = {Ieee},
title = {{Dimensionality Reduction using a Mixed Norm Penalty Function}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1532880},
year = {2005}
}
@article{Choi2008a,
abstract = {Most manifold learning methods consider only one similarity matrix to induce a low-dimensional manifold embedded in data space. In practice, however, we often use multiple sensors at a time so that each sensory information yields different similarity matrix derived from the same objects. In such a case, manifold integration is a desirable task, combining these similarity matrices into a compromise matrix that faithfully reflects multiple sensory information. A small number of methods exists for manifold integration, including a method based on reproducing kernel Krein space (RKKS) or DISTATIS, where the former is restricted to the case of only two manifolds and the latter considers a linear combination of normalized similarity matrices as a compromise matrix. In this paper we present a new manifold integration method, Markov random walk on multiple manifolds (RAMS), which integrates transition probabilities defined on each manifold to compute a compromise matrix. Numerical experiments confirm that RAMS finds more informative manifolds with a desirable projection property.},
author = {Choi, Heeyoul and Choi, Seungjin and Choe, Yoonsuck},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Choi, Choe - 2008 - Manifold integration with Markov random walks.pdf:pdf},
journal = {Aaai Conference On Artificial Intelligence},
pages = {424--429},
title = {{Manifold integration with Markov random walks}},
url = {http://portal.acm.org/citation.cfm?id=1620064},
year = {2008}
}
@inproceedings{Ham2005a,
abstract = {In this paper, we study a family of semisupervised learning algorithms for "aligning" di\#erent data sets that are characterized by the same underlying manifold. The optimizations of these algorithms are based on graphs that provide a discretized approximation to the manifold. Partial alignments of the data sets---obtained from prior knowledge of their manifold structure or from pairwise correspondences of subsets of labeled examples--- are completed by integrating supervised signals with unsupervised frameworks for manifold learning. As an illustration of this semisupervised setting, we show how to learn mappings between di\#erent data sets of images that are parameterized by the same underlying modes of variability (e.g., pose and viewing angle). The curse of dimensionality in these problems is overcome by exploiting the low dimensional structure of image manifolds.},
author = {Ham, Jihun and Lee, D and Saul, L.},
booktitle = {Proceedings of the Annual Conference on Uncertainty in Artificial Intelligence, Z. Ghahramani and R. Cowell, Eds},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ham, Lee, Saul - 2005 - Semisupervised alignment of manifolds.pdf:pdf},
pages = {120--127},
publisher = {Citeseer},
title = {{Semisupervised alignment of manifolds}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.59.8098\&amp;rep=rep1\&amp;type=pdf},
volume = {10},
year = {2005}
}
@inproceedings{Zhai2010,
author = {Zhai, D. and Li, B. and Chang, H. and Shan, S. and Chen, X. and Gao, W. and CAS, C.},
booktitle = {Proceedings of the British Machine Vision Conference},
doi = {10.5244/C.24.3},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhai et al. - 2010 - Manifold Alignment via Corresponding Projections.pdf:pdf},
isbn = {1-901725-40-5},
pages = {3.1--3.11},
publisher = {British Machine Vision Association},
title = {{Manifold Alignment via Corresponding Projections}},
url = {http://www.jdl.ac.cn/doc/2010/DemingZhai\_BMVC\_2010.pdf},
year = {2010}
}
@article{Saul2001,
author = {Saul, Lawrence K L.K. and Ave, Park and Park, Florham and Roweis, S.T. Sam T},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saul, Roweis - 2001 - An introduction to locally linear embedding.pdf:pdf},
journal = {Available from иид ЛЛлллК зКигжгвигК йЛ жгл зЛаа Л},
publisher = {Citeseer},
title = {{An introduction to locally linear embedding}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.123.7319\&amp;rep=rep1\&amp;type=pdf},
year = {2001}
}
@book{Shaw2009,
address = {New York, New York, USA},
author = {Shaw, Blake and Jebara, Tony},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
doi = {10.1145/1553374.1553494},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shaw, Jebara - 2009 - Structure preserving embedding.pdf:pdf},
isbn = {9781605585161},
month = jun,
pages = {1--8},
publisher = {ACM Press},
title = {{Structure preserving embedding}},
url = {http://portal.acm.org/citation.cfm?id=1553374.1553494},
year = {2009}
}
@article{Agrafiotis2003,
abstract = {We introduce stochastic proximity embedding (SPE), a novel self-organizing algorithm for producing meaningful underlying dimensions from proximity data. SPE attempts to generate low-dimensional Euclidean embeddings that best preserve the similarities between a set of related observations. The method starts with an initial configuration, and iteratively refines it by repeatedly selecting pairs of objects at random, and adjusting their coordinates so that their distances on the map match more closely their respective proximities. The magnitude of these adjustments is controlled by a learning rate parameter, which decreases during the course of the simulation to avoid oscillatory behavior. Unlike classical multidimensional scaling (MDS) and nonlinear mapping (NLM), SPE scales linearly with respect to sample size, and can be applied to very large data sets that are intractable by conventional embedding procedures. The method is programmatically simple, robust, and convergent, and can be applied to a wide range of scientific problems involving exploratory data analysis and visualization.},
author = {Agrafiotis, Dimitris K},
doi = {10.1002/jcc.10234},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrafiotis - 2003 - Stochastic proximity embedding.pdf:pdf},
issn = {0192-8651},
journal = {Journal of computational chemistry},
keywords = {combinatorial chemistry,data,data analysis,dimensionality reduction,feature extraction,mining,molecular descriptor,molecular diversity,molecular similarity,multidimensional scaling,nonlinear mapping,pattern recognition,sammon mapping,self-organizing,stochastic descent,stochastic proximity embedding},
month = jul,
number = {10},
pages = {1215--21},
pmid = {12820129},
title = {{Stochastic proximity embedding.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12820129},
volume = {24},
year = {2003}
}
@article{Xiong2007,
abstract = {. We study the problem of manifold alignment, which aims at “aligning” different data sets that share a similar intrinsic manifold provided some supervision. Unlike traditional methods that rely on pairwise correspondences between the two data sets, our method only needs some relative comparison information like “A is more similar to B than A is to C”. This method provides a more flexible way to acquire the prior knowledge for alignment, thus is able to handle situations where corresponding pairs are hard or impossible to identify. We optimize our objective based on the graphs that give discrete approximations of the manifold. Further, the problem is formulated as a semi-definite programming (SDP) problem which can readily be solved. Finally, experimental results are presented to show the effectiveness of our method. 1},
annote = {From Duplicate 1 ( },
author = {Xiong, Liang and Wang, Fei and Zhang, Changshui},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiong, Wang, Zhang - 2007 - Semi-definite Manifold Alignment.pdf:pdf;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiong, Wang, Zhang - 2007 - Semi-definite manifold alignment(2).pdf:pdf},
journal = {Machine Learning: ECML 2007},
pages = {773--781},
title = {{Semi-definite manifold alignment}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.89.4555 http://www.springerlink.com/index/q338x77086q83412.pdf},
year = {2007}
}
@article{Xiong,
abstract = {. We study the problem of manifold alignment, which aims at “aligning” different data sets that share a similar intrinsic manifold provided some supervision. Unlike traditional methods that rely on pairwise correspondences between the two data sets, our method only needs some relative comparison information like “A is more similar to B than A is to C”. This method provides a more flexible way to acquire the prior knowledge for alignment, thus is able to handle situations where corresponding pairs are hard or impossible to identify. We optimize our objective based on the graphs that give discrete approximations of the manifold. Further, the problem is formulated as a semi-definite programming (SDP) problem which can readily be solved. Finally, experimental results are presented to show the effectiveness of our method. 1},
annote = {From Duplicate 2 ( },
author = {Xiong, Liang and Wang, Fei and Zhang, Changshui},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiong, Wang, Zhang - 2007 - Semi-definite Manifold Alignment.pdf:pdf;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiong, Wang, Zhang - 2007 - Semi-definite manifold alignment(2).pdf:pdf;:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiong, Wang, Zhang - 2007 - Semi-definite Manifold Alignment(3).pdf:pdf},
journal = {Machine Learning: ECML 2007},
pages = {773--781},
title = {{Semi-definite Manifold Alignment}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.89.4555 http://www.springerlink.com/index/q338x77086q83412.pdf},
year = {2007}
}
@inproceedings{Verbeek2004,
author = {Verbeek, J.J. and Roweis, S.T. and Vlassis, Nikos},
booktitle = {Advances in neural information processing systems 16: proceedings of the 2003 conference},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Verbeek, Roweis, Vlassis - 2004 - Non-linear CCA and PCA by alignment of local models.pdf:pdf},
pages = {297},
publisher = {The MIT Press},
title = {{Non-linear CCA and PCA by alignment of local models}},
url = {http://books.google.com/books?hl=en\&amp;lr=\&amp;id=0F-9C7K8fQ8C\&amp;oi=fnd\&amp;pg=PA297\&amp;dq=Non-linear+CCA+and+PCA+by+Alignment+of+Local+Models\&amp;ots=TGExjTQe7Z\&amp;sig=frZGqP2GAOAnaRM4YnP00g\_7yGA},
volume = {16},
year = {2004}
}
@article{Agrawal2005,
author = {Agrawal, Rakesh and Gehrke, Johannes and Gunopulos, Dimitrios and Raghavan, Prabhakar},
doi = {10.1007/s10618-005-1396-1},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal et al. - 2005 - Automatic Subspace Clustering of High Dimensional Data.pdf:pdf},
issn = {1384-5810},
journal = {Data Mining and Knowledge Discovery},
keywords = {clustering,dimensionality reduction,subspace clustering},
month = jul,
number = {1},
pages = {5--33},
title = {{Automatic Subspace Clustering of High Dimensional Data}},
url = {http://www.springerlink.com/index/10.1007/s10618-005-1396-1},
volume = {11},
year = {2005}
}
@phdthesis{Wang2010,
author = {Wang, Chang},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang - 2010 - A geometric framework for transfer learning using manifold alignment.pdf:pdf},
keywords = {Manifold Alignment,Representat,Transfer Learning},
number = {September},
publisher = {Citeseer},
title = {{A geometric framework for transfer learning using manifold alignment}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.6983\&amp;rep=rep1\&amp;type=pdf},
year = {2010}
}
@article{Loh2012,
author = {Loh, Po-Ling and Wainwright, Martin J.},
issn = {2168-8966},
journal = {The Annals of Statistics},
keywords = {\$M\$-estimation,High-dimensional statistics,missing data,nonconvexity,regularization,sparse linear regression},
language = {EN},
month = jun,
number = {3},
pages = {1637--1664},
title = {{High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity}},
url = {http://projecteuclid.org/euclid.aos/1346850068},
volume = {40},
year = {2012}
}
@article{Georgiev2012,
abstract = {A variety of dimension reduction methods can be framed as (generalized) eigenvalue decomposition problems. For these methods we develop an algorithmic framework for inference of low-dimensional structure from massive high-dimensional data by leveraging recently developed highly scalable randomized low-rank approximation approaches. We propose efficient randomized algorithms for supervised and unsupervised (non)linear dimension reduction - specifically, (localized) Sliced Inverse Regression (SIR), Locality Preserving Projections (LPP), and Principal Components Analysis (PCA). A key point in our development is the emphasis on the dual role of randomization as simultaneously enhancing the computational feasibility while introducing regularization in the dimension reduction estimators. This point is highlighted by results on simulated and real data that show improved performance for the randomized over the exact methods.},
archivePrefix = {arXiv},
arxivId = {1211.1642},
author = {Georgiev, Stoyan and Mukherjee, Sayan},
eprint = {1211.1642},
month = nov,
pages = {36},
title = {{Randomized Algorithms for Dimension Reduction on Massive Data}},
url = {http://arxiv.org/abs/1211.1642},
year = {2012}
}
@article{Verma2013,
author = {Verma, Nakul},
file = {:C$\backslash$:/Users/sadali/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Verma - 2013 - Distance Preserving Embeddings for General n-Dimensional Manifolds.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {embedding},
mendeley-tags = {embedding},
pages = {2415--2448},
title = {{Distance Preserving Embeddings for General n-Dimensional Manifolds}},
url = {http://jmlr.org/papers/v14/verma13a.html},
volume = {14},
year = {2013}
}
